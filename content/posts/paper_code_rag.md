---
title: "RAG论文精读"
date: 2024-09-16T17:23:37+08:00
draft: false
author: "hxy"
categories: ["科研"]
tags: ["论文摘要"]
---
## 1.《CODERAG-BENCH: Can Retrieval Augment Code Generation?》
### 前置条件：
语言模型已经被证明具有优秀的生成代码能力，RAG在文本任务中取得成功。若将两者合在一起能有好的效果吗？
### 论文的工作：
探索在何种场景下检索能够有益于代码生成模型？存在什么挑战？
首先策划了一个综合评估基准，CODERAG-BENCH。
三类代码生成任务：包括**基础编程、开放领域、库级别的问题**。
将五个方面的文档聚合起来检索上下文。
### 发现：
当前的检索器存在困难去抓取上下文。希望CODERAG-BENCH可以作为一个有效的测试台，以鼓励进一步开发高级面向代码的RAG方法。
### 思考：
和夏令营任务相结合，此时我们的任务是生成循环代码。依靠的上下文是毕设论文中研究出来的数据集。

### 研究过程
共收集了九千代码问题和25000000检索文档，
CODERAG-BENCH的特点：任务多样性、严谨可复现的评估、统一接口。
RAG减少了在模型参数[2]中包含所有知识的需要，从而在各种场景[13]中提高精度，即使没有额外的训练[31,25]。

### 论文贡献：
提出了全新的评估基准（CODERAG-BENCH），来弥补鸿沟并增强在替代范式检索增强代码生成的研究。

NDCG、Precision和Recall

尽管代码生成模型可以从多个场景中的地面真实文档中获益，但当前的检索模型在选择准确的文档方面存在困难，特别是对于开放域任务。与此同时，许多代码生成模型由于使用检索到的文档的上下文能力有限，或者有效地执行RACG的能力有限，因此获益甚微。

（i）不同的任务：代码生成涉及在不同级别（行、功能、存储库）和不同域（关闭、打开）上操作的通用任务。
（ii）严格和可重复性的评估：我们提供高质量的地面真实文档注释，以实现检索评估，并对所有代码生成任务进行基于执行的评估，以严格衡量功能的正确性。（iii）统一接口：虽然当前的数据集使用异构管道，但我们的代码库为检索、增强生成和评估提供了一个统一的接口。
## 2.《Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization)》
大语言模型提示词自动语义增强（用于代码摘要）

### 论文贡献
1. 使用来自代码的事实进行软件工程任务的ASAP方法。
2. 我们对代码-002、文本-003上的代码总结任务进行了评估。和GPT-3.5-turbo模型对抗使用香草BM25构建的几个镜头提示基线（第4.1节）。
3. 我们发现，ASAP方法在统计学上显著提高了在代码摘要任务上的LLM性能。在几乎所有的情况下，我们观察到几乎或超过2个BLEU的统计学显著改善；对于PHP，我们（据我们所知）在这个具有挑战性的数据集上首次打破了30个BLEU。
4. 我们发现，ASAP也可以提高代码完成任务的性能。

## 3.《From Local to Global: A Graph RAG Approach to Query-Focused Summarization》
图RAG方法，用于聚焦于询问的摘要
Query-Focused Summarization: QFS 
QFS 会根据用户提供的查询，提取与之相关的核心信息，省略与查询无关的内容。

### 论文核心图
![](https://image.familystudy.cn/image/generic/image-1729089195232-2.png)

### 实现过程

提出了Graph RAG方法，基于LLM汲取知识图谱的全局摘要。
为了评估这个方法，我们使用一个LLM去生成多样的集合，包含以活动为中心的意义构建问题，从短小的描述

中间的、低等级的社区摘要表明优秀的性能，超过源文本摘要在相同的metrics，并消耗较少token。

### 构建步骤：
1. 源文档->文本语料库
2. 文本语料库->元素实例
3. 元素实例->元素摘要
4. 元素摘要->图社区
